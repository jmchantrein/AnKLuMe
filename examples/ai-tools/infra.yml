---
# AI Tools Domain — Full local AI stack
#
# This example deploys a complete AI tools domain with:
# - Ollama (LLM inference with GPU)
# - Open WebUI (chat interface)
# - LobeChat (alternative chat UI with multi-provider support)
# - OpenCode (AI coding assistant server)
# - Speaches STT (speech-to-text)
#
# All AI services connect to the local Ollama instance.
# The domain is isolated from other domains by default.
# Use network_policies to allow cross-domain access.
#
# IPs: anklume -> 10.100.0.x (admin), ai-tools -> 10.120.0.x (semi-trusted)
#
# Requirements:
# - NVIDIA GPU (recommended, CPU fallback available)
# - 16+ GB RAM (for LLM models + services)
# - 50+ GB disk (for models)

project_name: ai-stack

global:
  addressing:
    base_octet: 10
    zone_base: 100
  default_os_image: "images:debian/13"
  default_connection: community.general.incus
  default_user: root

domains:
  anklume:
    trust_level: admin
    description: "Administration domain"
    machines:
      anklume-instance:
        description: "Ansible controller"
        type: lxc
        roles: [base_system]

  ai-tools:
    trust_level: semi-trusted
    description: "AI services domain — LLM, chat, coding, STT"
    file_portal:
      allowed_paths: ["/shared/ai-tools", "/tmp/portal"]
      read_only: false
    profiles:
      nvidia-compute:
        devices:
          gpu:
            type: gpu
            gputype: physical
    machines:
      gpu-server:
        description: "Ollama LLM server + STT"
        type: lxc
        gpu: true
        profiles: [default, nvidia-compute]
        roles: [base_system, ollama_server, stt_server]
      ai-openwebui:
        description: "Open WebUI chat interface"
        type: lxc
        roles: [base_system, open_webui]
      ai-lobechat:
        description: "LobeChat multi-provider UI"
        type: lxc
        roles: [base_system, lobechat]
      ai-opencode:
        description: "OpenCode AI coding server"
        type: lxc
        roles: [base_system, opencode_server]
        export_apps: [opencode]
      ai-coder:
        description: "Sandboxed AI coding (Claude Code, Aider)"
        type: lxc
        roles: [base_system, code_sandbox]
      ai-openclaw:
        description: "OpenClaw self-hosted AI assistant"
        type: lxc
        config:
          security.nesting: "true"
        roles: [base_system, openclaw_server]

network_policies:
  - description: "Admin accesses AI chat UIs"
    from: anklume
    to: ai-tools
    ports: [3000, 3210, 4096]
    protocol: tcp

  - description: "Host accesses Ollama API directly"
    from: host
    to: gpu-server
    ports: [11434]
    protocol: tcp
