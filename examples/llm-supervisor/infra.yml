---
# infra.yml â€” Primary Source of Truth (PSOT)
#
# LLM supervisor: 2 isolated LLM domains + 1 supervisor domain.
# Each LLM runs in its own network. The supervisor communicates with
# both via API for monitoring and management.
#
# IPs: anklume -> 10.100.0.x (admin)
#       supervisor -> 10.110.0.x (trusted)
#       llm-alpha/llm-beta -> 10.120.x.x (semi-trusted)

project_name: llm-supervisor

global:
  addressing:
    base_octet: 10
    zone_base: 100
  default_os_image: "images:debian/13"
  default_connection: community.general.incus
  default_user: root
  gpu_policy: shared

domains:
  anklume:
    trust_level: admin
    description: "Administration and orchestration"
    machines:
      llms-admin:
        description: "Ansible controller"
        type: lxc
        config:
          security.nesting: "true"
        roles:
          - base_system

  supervisor:
    trust_level: trusted
    description: "Supervisor monitoring both LLMs"
    machines:
      llm-supervisor:
        description: "Supervisor container"
        type: lxc
        roles:
          - base_system
      llm-webui:
        description: "Open WebUI for interacting with LLMs"
        type: lxc
        roles:
          - base_system
          - open_webui

  llm-alpha:
    trust_level: semi-trusted
    description: "First LLM instance"
    profiles:
      nvidia-compute:
        devices:
          gpu:
            type: gpu
            gputype: physical
    machines:
      llm-alpha-server:
        description: "Ollama server (primary model)"
        type: lxc
        gpu: true
        profiles:
          - default
          - nvidia-compute
        roles:
          - base_system
          - ollama_server

  llm-beta:
    trust_level: semi-trusted
    description: "Second LLM instance"
    profiles:
      nvidia-compute:
        devices:
          gpu:
            type: gpu
            gputype: physical
    machines:
      llm-beta-server:
        description: "Ollama server (secondary model)"
        type: lxc
        gpu: true
        profiles:
          - default
          - nvidia-compute
        roles:
          - base_system
          - ollama_server
